{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'create_artist_name_col' from 'cleaning_scripts.string_operations' (/Users/CUDAN/Documents/GitHub/data_analysis_portfolio/cleaning_scripts/string_operations.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcleaning_scripts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mload_data_to_clean2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_data_to_clean\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcleaning_scripts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstring_operations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_artist_name_col\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'create_artist_name_col' from 'cleaning_scripts.string_operations' (/Users/CUDAN/Documents/GitHub/data_analysis_portfolio/cleaning_scripts/string_operations.py)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from cleaning_scripts.load_data_to_clean2 import load_data_to_clean\n",
    "from cleaning_scripts.string_operations import create_artist_name_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with open('cleaning_scripts/string_operations.py', 'r') as file:\n",
    "    print(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_artist_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 10\u001b[0m\n\u001b[1;32m      4\u001b[0m df_test \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124martist_name\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mArtist A\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mArtist B\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject_title\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mArtwork 1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mArtwork 2\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      7\u001b[0m })\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Call the function\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m df_test \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_artist_name\u001b[49m(df_test)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Check the result\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_test)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'create_artist_name' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame to test the function\n",
    "df_test = pd.DataFrame({\n",
    "    'artist_name': ['Artist A', 'Artist B'],\n",
    "    'object_title': ['Artwork 1', 'Artwork 2']\n",
    "})\n",
    "\n",
    "# Call the function\n",
    "df_test = create_artist_name(df_test)\n",
    "\n",
    "# Check the result\n",
    "print(df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded met with shape: (476318, 54)\n",
      "Loaded reina_sofia with shape: (10018, 16)\n",
      "Loaded tate with shape: (69201, 31)\n",
      "Loaded pompidou with shape: (115217, 23)\n",
      "Loaded moma with shape: (157474, 30)\n",
      "Loaded whitney with shape: (4012, 8)\n",
      "Loaded national_gallery with shape: (141438, 55)\n",
      "Loaded flattened data with shape: (58264, 499)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m raw_data, dataset_names \u001b[38;5;241m=\u001b[39m \u001b[43mload_data_to_clean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/data_analysis_portfolio/cleaning_scripts/load_data_to_clean2.py:56\u001b[0m, in \u001b[0;36mload_data_to_clean\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m df \u001b[38;5;129;01min\u001b[39;00m dictionary_data:\n\u001b[1;32m     55\u001b[0m     flattened_data \u001b[38;5;241m=\u001b[39m [flatten(record) \u001b[38;5;28;01mfor\u001b[39;00m record \u001b[38;5;129;01min\u001b[39;00m df]\n\u001b[0;32m---> 56\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflattened_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m     flat_data\u001b[38;5;241m.\u001b[39mappend(data)\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded flattened data with shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/frame.py:806\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    804\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    805\u001b[0m         columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[0;32m--> 806\u001b[0m     arrays, columns, index \u001b[38;5;241m=\u001b[39m \u001b[43mnested_data_to_arrays\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    807\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001b[39;49;00m\n\u001b[1;32m    808\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001b[39;49;00m\n\u001b[1;32m    809\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    811\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    812\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    813\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    814\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m arrays_to_mgr(\n\u001b[1;32m    815\u001b[0m         arrays,\n\u001b[1;32m    816\u001b[0m         columns,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    819\u001b[0m         typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[1;32m    820\u001b[0m     )\n\u001b[1;32m    821\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/internals/construction.py:520\u001b[0m, in \u001b[0;36mnested_data_to_arrays\u001b[0;34m(data, columns, index, dtype)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_named_tuple(data[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;129;01mand\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    518\u001b[0m     columns \u001b[38;5;241m=\u001b[39m ensure_index(data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_fields)\n\u001b[0;32m--> 520\u001b[0m arrays, columns \u001b[38;5;241m=\u001b[39m \u001b[43mto_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    521\u001b[0m columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/internals/construction.py:845\u001b[0m, in \u001b[0;36mto_arrays\u001b[0;34m(data, columns, dtype)\u001b[0m\n\u001b[1;32m    842\u001b[0m     data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mtuple\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m    843\u001b[0m     arr \u001b[38;5;241m=\u001b[39m _list_to_arrays(data)\n\u001b[0;32m--> 845\u001b[0m content, columns \u001b[38;5;241m=\u001b[39m \u001b[43m_finalize_columns_and_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m content, columns\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/internals/construction.py:945\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[0;34m(content, columns, dtype)\u001b[0m\n\u001b[1;32m    942\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    944\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(contents) \u001b[38;5;129;01mand\u001b[39;00m contents[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mobject_:\n\u001b[0;32m--> 945\u001b[0m     contents \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_object_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m contents, columns\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/internals/construction.py:1068\u001b[0m, in \u001b[0;36mconvert_object_array\u001b[0;34m(content, dtype, dtype_backend, coerce_float)\u001b[0m\n\u001b[1;32m   1064\u001b[0m             arr \u001b[38;5;241m=\u001b[39m maybe_cast_to_datetime(arr, dtype)\n\u001b[1;32m   1066\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\n\u001b[0;32m-> 1068\u001b[0m arrays \u001b[38;5;241m=\u001b[39m [convert(arr) \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m content]\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arrays\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/internals/construction.py:1068\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1064\u001b[0m             arr \u001b[38;5;241m=\u001b[39m maybe_cast_to_datetime(arr, dtype)\n\u001b[1;32m   1066\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\n\u001b[0;32m-> 1068\u001b[0m arrays \u001b[38;5;241m=\u001b[39m [\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m content]\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arrays\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/internals/construction.py:1030\u001b[0m, in \u001b[0;36mconvert_object_array.<locals>.convert\u001b[0;34m(arr)\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(arr):\n\u001b[1;32m   1029\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1030\u001b[0m         arr \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaybe_convert_objects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m            \u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtry_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoerce_float\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1033\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert_to_nullable_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnumpy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1034\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1035\u001b[0m         \u001b[38;5;66;03m# Notes on cases that get here 2023-02-15\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m         \u001b[38;5;66;03m# 1) we DO get here when arr is all Timestamps and dtype=None\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m         \u001b[38;5;66;03m# 2) disabling this doesn't break the world, so this must be\u001b[39;00m\n\u001b[1;32m   1038\u001b[0m         \u001b[38;5;66;03m#    getting caught at a higher level\u001b[39;00m\n\u001b[1;32m   1039\u001b[0m         \u001b[38;5;66;03m# 3) passing convert_non_numeric to maybe_convert_objects get this right\u001b[39;00m\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;66;03m# 4) convert_non_numeric?\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "raw_data, dataset_names = load_data_to_clean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create functions for cleaning fields.\n",
    "artist name, artwork title, artwork medium, and acquisition source, aquisition year, creation year, artist nationality, artist gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'raw_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m transformed_data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m df \u001b[38;5;129;01min\u001b[39;00m \u001b[43mraw_data\u001b[49m:\n\u001b[1;32m      3\u001b[0m     create_artist_name(df)\n\u001b[1;32m      4\u001b[0m     transformed_data\u001b[38;5;241m.\u001b[39mappend(df)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'raw_data' is not defined"
     ]
    }
   ],
   "source": [
    "transformed_data = []\n",
    "for df in raw_data:\n",
    "    create_artist_name(df)\n",
    "    transformed_data.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_artist_name(df):\n",
    "    # List of possible column names for the artist name\n",
    "    possible_columns = ['artist', 'author', 'artist_name', 'Artist Display Name', 'artist name', 'Artist Display Name', 'Artist', 'artists', \n",
    "                        'forwarddisplayname', 'production_0_creator']\n",
    "    \n",
    "    # Loop over the possible column names and use the first one that exists\n",
    "    for col in possible_columns:\n",
    "        if col in df.columns:\n",
    "            df['Artist'] = df[col]\n",
    "            break  # Exit loop once we find the first match\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_artwork_title(df):\n",
    "    # List of possible column names for the artist name\n",
    "    possible_columns = ['object_title', 'artwork_name', 'Title', 'url', 'name', 'title', 'title_fi', 'titles_0_title']\n",
    "    \n",
    "    # Loop over the possible column names and use the first one that exists\n",
    "    for col in possible_columns:\n",
    "        if col in df.columns:\n",
    "            df['Title'] = df[col]\n",
    "            break  # Exit loop once we find the first match\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extract_first_number import extract_first_number\n",
    "\n",
    "    \n",
    "def clean_acquisition_year(df):\n",
    "   # List of possible column names for the artist name\n",
    "    possible_columns = ['acquisition_date', 'credit_line', 'AccessionYear', 'artwork_acquisition', 'year_adquisition', 'acquisitionYear',\n",
    "                        'DateAcquired', 'accession_number', 'accessionnum', 'inventoryNumber', 'acquisition_date_precision', 'AcquiredDate' ]\n",
    "    df['Year_acquisition'] = pd.NA\n",
    "  \n",
    "    # Loop over the possible column names and use the first one that exists\n",
    "    for col in possible_columns:\n",
    "        if col in df.columns:\n",
    "            df['Year_acquisition'] = df[col].apply(extract_first_number)\n",
    "            break  # Exit loop once we find the first match\n",
    "    \n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I need to add more tags based on dataset values\n",
    "from medium_tags import load_medium_tags #returns medium_tags, medium_name\n",
    "medium_tags, medium_name = load_medium_tags()\n",
    "def classify_medium(df):\n",
    "    df['Medium_classified'] = pd.NA\n",
    "    \n",
    "     # Iterate over each row in the DataFrame\n",
    "    for idx, row in df.iterrows():\n",
    "        medium = row['Medium_raw'] \n",
    "        \n",
    "        # Skip if the 'Medium' value is not a string (e.g., NaN or other types)\n",
    "        if not isinstance(medium, str):\n",
    "            continue\n",
    "        \n",
    "        # Check each tag group (e.g., ['paper', 'watercolor'], ['oil'])\n",
    "        for i, tag_group in enumerate(medium_tags):\n",
    "            # Check if any keyword from the tag group exists in the 'Medium' column\n",
    "            if any(tag.lower() in medium.lower() for tag in tag_group):\n",
    "                # Assign the corresponding medium name (from medium_name list)\n",
    "                df.at[idx, 'Medium_classified'] = medium_name[i]\n",
    "                break  # Stop once we find a match\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#i need to update the dictionary so all nationalities will be translated into countries\n",
    "from load_nationality_tags import load_nationality_tags\n",
    "nationality_tags, country_name = load_nationality_tags()\n",
    "def create_artist_nationality(df):\n",
    "    possible_columns = ['Country','Artist Nationality', 'nationality_artist', 'artist_nationality', 'Nationality', 'nationality', \n",
    "                        'production_0_creator_nationality']\n",
    "    df['Country_calculated'] = pd.NA\n",
    "    df['nationality_raw'] = pd.NA\n",
    "\n",
    "    # Check each possible column for nationality information\n",
    "    for col in possible_columns:\n",
    "        if col in df.columns:\n",
    "            df['nationality_raw'] = df[col].str.lower().str.strip('the').str.replace('(', '').str.replace(')', '').str.split(' ').str[0].str.strip(',')  # Copy the nationality info into 'nationality_raw'\n",
    "            break  # Exit loop once we find the first match \n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        nationality = row['nationality_raw'] \n",
    "        \n",
    "        # Skip if the 'nationality' value is not a string (e.g., NaN or other types)\n",
    "        if not isinstance(nationality, str):\n",
    "            continue\n",
    "        \n",
    "        # Check each tag group (e.g., ['paper', 'watercolor'], ['oil'])\n",
    "        for i, tag_group in enumerate(nationality_tags):\n",
    "            # Check if any keyword from the tag group exists in the 'nationality' column\n",
    "            if any(tag.lower() in nationality.lower() for tag in tag_group):\n",
    "                # Assign the corresponding nationality name (from nationality_name list)\n",
    "                df.at[idx, 'Country_calculated'] = country_name[i]\n",
    "                break  # Stop once we find a match\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use functions for raw datasets and dreate a new list with clean datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_artist_gender(df):\n",
    "    possible_columns = ['Artist Gender', 'gender', 'artist_gender', 'Gender', 'production_0_creator_gender']\n",
    "    # Define a gender dictionary to map common terms to 'female' or 'male'\n",
    "    gender_dict = {'female': [\"female\", 'woman'], 'male': ['male', 'man']}\n",
    "    \n",
    "    # Initialize the 'Gender_raw' column with missing values\n",
    "    df['Gender_raw'] = pd.NA\n",
    "    \n",
    "    # Check which column exists and assign to 'Gender_raw'\n",
    "    for col in possible_columns:\n",
    "        if col in df.columns:\n",
    "            df['Gender_raw'] = df[col]\n",
    "            break  # Exit loop once we find the first match\n",
    "    \n",
    "    # Clean the text: remove all non-alphanumeric characters, split by spaces, and use the first word\n",
    "    df['Gender_classified'] = df['Gender_raw'].str.replace(r'[^a-zA-Z\\s]', '', regex=True)  # Remove non-alphabetic characters\n",
    "    df['Gender_classified'] = df['Gender_classified'].str.split().str[0].str.lower()  # Split by space and take the first word\n",
    "\n",
    "    # Manually map \"man\" and \"woman\" to \"male\" and \"female\" directly\n",
    "    df['Gender_classified'] = df['Gender_classified'].replace({'man': 'male', 'woman': 'female'})\n",
    "    \n",
    "    # Use a safe check to apply gender classification only on valid strings\n",
    "    df['Gender_classified'] = df['Gender_classified'].apply(\n",
    "        lambda x: 'female' if isinstance(x, str) and 'female' in x else ('male' if isinstance(x, str) and 'male' in x else x)\n",
    "    )\n",
    "    \n",
    "    # Return the modified DataFrame\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = transformed_data[10][['acquisition_raw', 'Acquistion_classified']].drop_duplicates(subset=['acquisition_raw'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_acquisition_tags import load_acquisition_tags\n",
    "acqusition_tags, acquisition_methods = load_acquisition_tags()\n",
    "def create_acquisition_method(df):\n",
    "    possible_columns = ['Credit Line', 'creditLine', 'acquisition_type', 'CreditLine', 'credit_line', 'creditline', 'CreditLine']\n",
    "     # Initialize the 'Gender_raw' column with missing values\n",
    "    df['acquisition_raw'] = pd.NA\n",
    "    \n",
    "    # Check which column exists and assign to 'Gender_raw'\n",
    "    for col in possible_columns:\n",
    "        if col in df.columns:\n",
    "            df['acquisition_raw'] = df[col].str.lower()\n",
    "            break  # Exit loop once we find the first match\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        acquisition = row['acquisition_raw'] \n",
    "        \n",
    "        # Skip if the 'acquisition' value is not a string (e.g., NaN or other types)\n",
    "        if not isinstance(acquisition, str):\n",
    "            continue\n",
    "        \n",
    "        # Check each tag group \n",
    "        for i, tag_group in enumerate(acqusition_tags):\n",
    "            # Check if any keyword from the tag group exists in the 'acquisition' column\n",
    "            if any(tag.lower() in acquisition.lower() for tag in tag_group):\n",
    "                # Assign the corresponding acquisition name (from acquisition_name list)\n",
    "                df.at[idx, 'Acquistion_classified'] = acquisition_methods[i]\n",
    "                break  # Stop once we find a match\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#acquisition date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def artwork_creation_date(df):\n",
    "    possible_columns = ['Object Date', 'year_production', 'year', 'object_date', 'Date', 'display_date', 'endyear_x', 'yearFrom', \n",
    "                        'production_date_0_end', 'DateCreated']\n",
    "    \n",
    "    df['Date_creation_year'] = pd.NA\n",
    "  \n",
    "    # Loop over the possible column names and use the first one that exists\n",
    "    for col in possible_columns:\n",
    "        if col in df.columns:\n",
    "            df['Date_creation_year'] = df[col].apply(extract_first_number)\n",
    "            break  # Exit loop once we find the first match\n",
    "    \n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def artist_birth_year(df):\n",
    "    possible_columns = ['Artist Begin Date', 'author_born_year', 'yearOfBirth', 'artist_birth', 'BeginDate', 'birth_date', \n",
    "                        'beginyear', 'people_0_birthDate', 'production_0_creator_date_of_birth']\n",
    "    df['Artist_birth_year'] = pd.NA\n",
    "  \n",
    "    # Loop over the possible column names and use the first one that exists\n",
    "    for col in possible_columns:\n",
    "        if col in df.columns:\n",
    "            df['Artist_birth_year'] = df[col].apply(extract_first_number)\n",
    "            break  # Exit loop once we find the first match\n",
    "    \n",
    "    return df \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def artist_death_year(df):\n",
    "    possible_columns = ['Artist End Date', 'author_death_year', 'yearOfDeath', 'artist_death', 'EndDate', 'death_date', 'endyear', \n",
    "                        'people_0_deathDate', 'production_0_creator_date_of_death']\n",
    "    df['Artist_death_year'] = pd.NA\n",
    "  \n",
    "    # Loop over the possible column names and use the first one that exists\n",
    "    for col in possible_columns:\n",
    "        if col in df.columns:\n",
    "            df['Artist_death_year'] = df[col].apply(extract_first_number)\n",
    "            break  # Exit loop once we find the first match\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_data = []\n",
    "for df in raw_data:\n",
    "    create_artist_name(df)\n",
    "    create_artwork_title(df)\n",
    "    create_artwork_medium(df)\n",
    "    classify_medium(df)\n",
    "    create_artist_nationality(df)\n",
    "    artwork_creation_date(df)\n",
    "    clean_acquisition_year(df)\n",
    "    artist_birth_year(df)\n",
    "    artist_death_year(df)\n",
    "    create_artist_gender(df)\n",
    "    create_acquisition_method(df)\n",
    "    transformed_data.append(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Object Number', 'Is Highlight', 'Is Timeline Work', 'Is Public Domain',\n",
       "       'Object ID', 'Gallery Number', 'Department', 'AccessionYear',\n",
       "       'Object Name', 'Title', 'Culture', 'Period', 'Dynasty', 'Reign',\n",
       "       'Portfolio', 'Constituent ID', 'Artist Role', 'Artist Prefix',\n",
       "       'Artist Display Name', 'Artist Display Bio', 'Artist Suffix',\n",
       "       'Artist Alpha Sort', 'Artist Nationality', 'Artist Begin Date',\n",
       "       'Artist End Date', 'Artist Gender', 'Artist ULAN URL',\n",
       "       'Artist Wikidata URL', 'Object Date', 'Object Begin Date',\n",
       "       'Object End Date', 'Medium', 'Dimensions', 'Credit Line',\n",
       "       'Geography Type', 'City', 'State', 'County', 'Country', 'Region',\n",
       "       'Subregion', 'Locale', 'Locus', 'Excavation', 'River', 'Classification',\n",
       "       'Rights and Reproduction', 'Link Resource', 'Object Wikidata URL',\n",
       "       'Metadata Date', 'Repository', 'Tags', 'Tags AAT URL',\n",
       "       'Tags Wikidata URL', 'Artist', 'Medium_raw', 'Medium_classified',\n",
       "       'Country_calculated', 'nationality_raw', 'Date_creation_year',\n",
       "       'Year_acquisition', 'Artist_birth_year', 'Artist_death_year',\n",
       "       'Gender_raw', 'Gender_classified', 'acquisition_raw',\n",
       "       'Acquistion_classified'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_data[0].columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save clean datasets in separate csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "clean_data = []\n",
    "\n",
    "# Define the desired columns\n",
    "required_columns = ['Artist', 'Title', 'Medium', 'Medium_classified', 'Acquistion_classified', \n",
    "                    'Year_acquisition', 'Gender_classified', 'Artist_birth_year', \n",
    "                    'Artist_death_year', 'Country_calculated', 'Date_creation_year']\n",
    "\n",
    "for df in transformed_data:\n",
    "    # Add any missing columns as NaN\n",
    "    for col in required_columns:\n",
    "        if col not in df.columns:\n",
    "            df[col] = pd.NA  # or use 'None' or 'NaN' depending on your preference\n",
    "\n",
    "    clean_data.append(df[required_columns])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved met.csv\n",
      "Saved reina_sofia.csv\n",
      "Saved tate.csv\n",
      "Saved pompidou.csv\n",
      "Saved moma.csv\n",
      "Saved whitney.csv\n",
      "Saved national_gallery.csv\n",
      "Saved kiasma.csv\n",
      "Saved smk.csv\n",
      "Saved ateneum.csv\n",
      "Saved queensland.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Save each dataset using the corresponding name\n",
    "for dataset, name in zip(clean_data, dataset_names):\n",
    "    # Generate a unique filename using the dataset name\n",
    "    file_name = f\"{name}.csv\"\n",
    "    \n",
    "    # Save the DataFrame to a CSV file\n",
    "    dataset.to_csv(file_name, index=False)\n",
    "    print(f\"Saved {file_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
