{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from flatten_json import flatten\n",
    "import json\n",
    "from load_data_to_clean import load_data_to_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded met with shape: (476318, 54)\n",
      "Loaded reina_sofia with shape: (10018, 16)\n",
      "Loaded tate with shape: (69201, 31)\n",
      "Loaded pompidou with shape: (115217, 23)\n",
      "Loaded moma with shape: (157457, 30)\n",
      "Loaded whitney with shape: (26495, 20)\n",
      "Loaded national_gallery with shape: (141430, 55)\n",
      "Loaded flattened data with shape: (58264, 499)\n",
      "Loaded flattened data with shape: (175000, 2852)\n",
      "Loaded flattened data with shape: (58264, 499)\n",
      "Loaded queensland with shape: (20250, 17)\n"
     ]
    }
   ],
   "source": [
    "raw_data, dataset_names = load_data_to_clean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create functions for cleaning fields.\n",
    "artist name, artwork title, artwork medium, and acquisition source, aquisition year, creation year, artist nationality, artist gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_artist_name(df):\n",
    "    # List of possible column names for the artist name\n",
    "    possible_columns = ['artist', 'author', 'artist_name', 'Artist Display Name', 'artist name', 'Artist Display Name', 'Artist', 'artists', \n",
    "                        'forwarddisplayname', 'production_0_creator']\n",
    "    \n",
    "    # Loop over the possible column names and use the first one that exists\n",
    "    for col in possible_columns:\n",
    "        if col in df.columns:\n",
    "            df['Artist'] = df[col]\n",
    "            break  # Exit loop once we find the first match\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_artwork_title(df):\n",
    "    # List of possible column names for the artist name\n",
    "    possible_columns = ['object_title', 'artwork_name', 'Title', 'url', 'name', 'title', 'title_fi', 'titles_0_title']\n",
    "    \n",
    "    # Loop over the possible column names and use the first one that exists\n",
    "    for col in possible_columns:\n",
    "        if col in df.columns:\n",
    "            df['Title'] = df[col]\n",
    "            break  # Exit loop once we find the first match\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extract_first_number import extract_first_number\n",
    "\n",
    "    \n",
    "def clean_acquisition_year(df):\n",
    "   # List of possible column names for the artist name\n",
    "    possible_columns = ['acquisition_date', 'credit_line', 'AccessionYear', 'artwork_acquisition', 'year_adquisition', 'acquisitionYear',\n",
    "                        'DateAcquired', 'accession_number', 'accessionnum', 'inventoryNumber', 'acquisition_date_precision', 'AcquiredDate' ]\n",
    "    df['Year_acquisition'] = pd.NA\n",
    "  \n",
    "    # Loop over the possible column names and use the first one that exists\n",
    "    for col in possible_columns:\n",
    "        if col in df.columns:\n",
    "            df['Year_acquisition'] = df[col].apply(extract_first_number)\n",
    "            break  # Exit loop once we find the first match\n",
    "    \n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_artwork_medium(df):\n",
    "    # List of possible column names for the artist name\n",
    "    possible_columns = ['Classification', 'classification', 'classifications_0_en','object_type', 'type_artwork', 'artwork_medium', 'Medium', 'medium',\n",
    "                        'object_names_1_name', 'PhysicalCategory']\n",
    "    # Loop over the possible column names and use the first one that exists\n",
    "    for col in possible_columns:\n",
    "        if col in df.columns:\n",
    "            df['Medium_raw'] = df[col]\n",
    "            break  # Exit loop once we find the first match\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I need to add more tags based on dataset values\n",
    "from medium_tags import load_medium_tags #returns medium_tags, medium_name\n",
    "medium_tags, medium_name = load_medium_tags()\n",
    "def classify_medium(df):\n",
    "    df['Medium_classified'] = pd.NA\n",
    "    \n",
    "     # Iterate over each row in the DataFrame\n",
    "    for idx, row in df.iterrows():\n",
    "        medium = row['Medium_raw'] \n",
    "        \n",
    "        # Skip if the 'Medium' value is not a string (e.g., NaN or other types)\n",
    "        if not isinstance(medium, str):\n",
    "            continue\n",
    "        \n",
    "        # Check each tag group (e.g., ['paper', 'watercolor'], ['oil'])\n",
    "        for i, tag_group in enumerate(medium_tags):\n",
    "            # Check if any keyword from the tag group exists in the 'Medium' column\n",
    "            if any(tag.lower() in medium.lower() for tag in tag_group):\n",
    "                # Assign the corresponding medium name (from medium_name list)\n",
    "                df.at[idx, 'Medium_classified'] = medium_name[i]\n",
    "                break  # Stop once we find a match\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = transformed_data[1][['nationality_raw', 'Country_calculated']].drop_duplicates(subset=['nationality_raw'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['queensland australia ', 'new south wales australia ',\n",
       "       'lithuania/australia vic ', ..., 'togo-brisby, jasmine - creator',\n",
       "       'savage, paula - creator', 'ryui, koji - creator'], dtype=object)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_data[10]['nationality_raw'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#i need to update the dictionary so all nationalities will be translated into countries\n",
    "from load_nationality_tags import load_nationality_tags\n",
    "nationality_tags, country_name = load_nationality_tags()\n",
    "def create_artist_nationality(df):\n",
    "    possible_columns = ['Country','Artist Nationality', 'nationality_artist', 'artist_nationality', 'Nationality', 'nationality', \n",
    "                        'production_0_creator_nationality']\n",
    "    df['Country_calculated'] = pd.NA\n",
    "    df['nationality_raw'] = pd.NA\n",
    "\n",
    "    # Check each possible column for nationality information\n",
    "    for col in possible_columns:\n",
    "        if col in df.columns:\n",
    "            df['nationality_raw'] = df[col].str.replace('(', '').str.replace(')', '').str.lower()  # Copy the nationality info into 'nationality_raw'\n",
    "            break  # Exit loop once we find the first match \n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        nationality = row['nationality_raw'] \n",
    "        \n",
    "        # Skip if the 'nationality' value is not a string (e.g., NaN or other types)\n",
    "        if not isinstance(nationality, str):\n",
    "            continue\n",
    "        \n",
    "        # Check each tag group (e.g., ['paper', 'watercolor'], ['oil'])\n",
    "        for i, tag_group in enumerate(nationality_tags):\n",
    "            # Check if any keyword from the tag group exists in the 'nationality' column\n",
    "            if any(tag.lower() in nationality.lower() for tag in tag_group):\n",
    "                # Assign the corresponding nationality name (from nationality_name list)\n",
    "                df.at[idx, 'Country_calculated'] = country_name[i]\n",
    "                break  # Stop once we find a match\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use functions for raw datasets and dreate a new list with clean datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_artist_gender(df):\n",
    "    possible_columns = ['Artist Gender', 'gender', 'artist_gender', 'Gender', 'production_0_creator_gender']\n",
    "    # Loop over the possible column names and use the first one that exists\n",
    "    gender_dict = {'Female':\"female\",\n",
    "                   'male':'male'}\n",
    "    for col in possible_columns:\n",
    "        if col in df.columns:\n",
    "            df['Gender_calculated'] = df[col].replace(gender_dict)\n",
    "            break  # Exit loop once we find the first match\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_acquisition_method(df):\n",
    "    possible_columns = ['Credit Line', 'creditLine', 'acquisition_type', 'CreditLine', 'credit_line', 'creditline', 'CreditLine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def artwork_creation_date(df):\n",
    "    possible_columns = ['Object Date', 'year_production', 'year', 'object_date', 'Date', 'display_date', 'endyear_x', 'yearFrom', \n",
    "                        'production_date_0_end', 'DateCreated']\n",
    "    \n",
    "    df['Date_creation_year'] = pd.NA\n",
    "  \n",
    "    # Loop over the possible column names and use the first one that exists\n",
    "    for col in possible_columns:\n",
    "        if col in df.columns:\n",
    "            df['Date_creation_year'] = df[col].apply(extract_first_number)\n",
    "            break  # Exit loop once we find the first match\n",
    "    \n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def artist_birth_year(df):\n",
    "    possible_columns = ['Artist Begin Date', 'author_born_year', 'yearOfBirth', 'artist_birth', 'BeginDate', 'birth_date', \n",
    "                        'beginyear', 'people_0_birthDate', 'production_0_creator_date_of_birth']\n",
    "    df['Artist_birth_year'] = pd.NA\n",
    "  \n",
    "    # Loop over the possible column names and use the first one that exists\n",
    "    for col in possible_columns:\n",
    "        if col in df.columns:\n",
    "            df['Artist_birth_year'] = df[col].apply(extract_numbers)\n",
    "            break  # Exit loop once we find the first match\n",
    "    \n",
    "    return df \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def artist_death_year(df):\n",
    "    possible_columns = ['Artist End Date', 'author_death_year', 'yearOfDeath', 'artist_death', 'EndDate', 'death_date', 'endyear', \n",
    "                        'people_0_deathDate', 'production_0_creator_date_of_death']\n",
    "    df['Artist_death_year'] = pd.NA\n",
    "  \n",
    "    # Loop over the possible column names and use the first one that exists\n",
    "    for col in possible_columns:\n",
    "        if col in df.columns:\n",
    "            df['Artist_death_year'] = df[col].apply(extract_numbers)\n",
    "            break  # Exit loop once we find the first match\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_data = []\n",
    "for df in raw_data:\n",
    "    #create_artist_name(df)\n",
    "    #create_artwork_title(df)\n",
    "    #create_artwork_medium(df)\n",
    "    #classify_medium(df)\n",
    "    create_artist_nationality(df)\n",
    "    #artwork_creation_date(df)\n",
    "    #clean_acquisition_year(df)\n",
    "    #artist_birth_year(df)\n",
    "    #artist_death_year(df)\n",
    "    #create_artist_gender(df)\n",
    "    transformed_data.append(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, '|', '|Female', '||', '||||', 'Female|', 'Female', '|||',\n",
       "       'Female|Female', '||Female', '|Female|Female', 'Female||',\n",
       "       'Female|Female|Female|Female|Female||Female|Female||||||Female|Female|Female|Female|||Female|||Female|Female|',\n",
       "       '|Female|', '|||Female', 'Female||Female', 'Female|Female|Female|',\n",
       "       '||||||||', '|||||||||||||||||||||||||||', 'Female|Female|',\n",
       "       'Female|Female||', 'Female|||', '|Female||', '|Female||Female',\n",
       "       '||Female|', 'Female|Female|Female', '||||||', '|Female|||Female',\n",
       "       '|||||', '||||Female|', '||||||||||', '||||||||||||||||',\n",
       "       '|||||||', '|||||||||||||', '|||||||||', '||||||||||||||||||||',\n",
       "       'Female|||||||||||||||||||||||||||||Female||Female||||',\n",
       "       'Female||||Female|||Female|||||||||||||||||Female|',\n",
       "       '|Female||||||||||||||Female', '||||||Female||||||',\n",
       "       'Female||||Female|||||Female|||',\n",
       "       '||||Female||||||Female|||||Female||||Female', '||||Female',\n",
       "       '||||||||||||||||||||||||||||||', '||Female||',\n",
       "       'Female|||||||Female|Female||||Female||', '|||||||||||||||||',\n",
       "       '||||||||||||||||||', '||Female|Female|Female|||||Female|||||',\n",
       "       '||||||||||||', '|||||||||||||||',\n",
       "       '||||||||||||||||||||||||||||||||||||||', '||Female||||||',\n",
       "       '||||||||||||||||||||||||||', 'Female|||Female|', '|||||||||||',\n",
       "       '||Female|||||||||||Female|||||||', '|||||||||Female',\n",
       "       '||||||||Female||||||||', '|Female||||||||||||||Female||||',\n",
       "       '|||Female|', '|||||||||||||Female||||||',\n",
       "       '|||||||||||||||||||||||||||||', '|||||||Female|Female',\n",
       "       '|||||||Female|', '||||||||||||||Female|||||||', '||||||||||||||',\n",
       "       '|||||||||Female|', '||Female||Female||', '|Female|Female|Female',\n",
       "       '|Female||||||||||||||||Female|||||||||||||||',\n",
       "       '|||||Female||||||||||||Female|||Female|||||||||',\n",
       "       '||||||||||||Female|||||Female||||||||||||||',\n",
       "       '|||||||||||||||||Female||||||||||||||||',\n",
       "       '|Female|||||||||||||||||||||||',\n",
       "       '|||||||||||||||||||||||||||||||||||||Female|||||||||||||||||||||||||',\n",
       "       '|Female||||||||||||||||||||||||||Female||||Female|||||||||||||||||',\n",
       "       '||||||||||||||||Female|||||||||||||', 'Female|||||Female|',\n",
       "       '||||||||||||||||||||Female||Female||||Female|Female|Female|||||||Female|||Female|Female|||||||||||||Female|||||||||||||||Female',\n",
       "       'Female||||||||||||', '||||||||||||||||||||||||||||||||',\n",
       "       '|||||||||||||||||||||||||', '|||||Female|||||||',\n",
       "       '|||||||||||||||||||||||||||||||||', '|||||Female||',\n",
       "       '|||Female|||||', '|||||Female', '||||||||||||||||||||||||',\n",
       "       '|||||||Female|||||||||||||||||||||||||',\n",
       "       '|||||||||||||||||||||||||||||||||||||', '|||||||||||||||||||',\n",
       "       '|||||Female||||Female|',\n",
       "       '||||||||Female|Female|Female|||||||||||||Female|||||',\n",
       "       '|||||||||Female|||||||||||||||||||||||', '|||Female||',\n",
       "       '|||||||||||||||||||||||||||||||||||||||||||||||||',\n",
       "       '||||||||Female||||', '||||||||||||||||||||||',\n",
       "       '|||||||||||||||||||||||||||||||||||||||||||||||', 'Female||||',\n",
       "       '||||||||||||Female|||||', '|Female||||||||',\n",
       "       '||||||||||||Female||||Female||||Female|||||||||||',\n",
       "       '||Female||||', '|||||||||||||||||||||||||||||||||||',\n",
       "       '||||Female|||', 'Female||Female|Female', 'Female|Female||Female',\n",
       "       '||||||||Female|', '|Female|Female|||Female', '||||||Female',\n",
       "       '||||||||||||||||||||||||||||', '|||Female|||',\n",
       "       '||Female|Female|Female|||||||Female||',\n",
       "       '|||||||||||||||||Female|||||', '||||Female|||||||',\n",
       "       '||||||||||||||||||||||||||||||Female|||||',\n",
       "       '|||||||||||||||||||||Female||||||||||||||||||||',\n",
       "       '|||||||||||||||||||||', '||||||||||||||||||||||||||Female|',\n",
       "       '||Female|||', '||||||Female||', '|||Female||Female|',\n",
       "       '||||||||||||||Female|||||Female||',\n",
       "       '||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||',\n",
       "       '|||||||Female', '|||||||||||||||||||||||||||||||||||||||||',\n",
       "       '||||||||Female|||||||',\n",
       "       '|||Female|||Female|Female||||||Female|||||||||||||||||',\n",
       "       '||||||||||||||||||||Female|||||||||||||',\n",
       "       '|||||||||||||||||||||||||||||||||||||||', '|||||Female|',\n",
       "       'Female||||||', '||||||||||||||Female|||||', '|||||Female||||||',\n",
       "       '|Female|||||', '||||Female|Female||||Female||||||||Female|||',\n",
       "       '|Female|||', '|||||||||||||Female|||', '||||||Female|||||',\n",
       "       '||Female||||||Female||||||||',\n",
       "       '||||||||||||||||||||||||||||||||||||||||||||||||||',\n",
       "       '|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||',\n",
       "       '||Female||||Female|Female', '|||||||||||||||Female||||',\n",
       "       '|||||||||||||||Female||Female||||||||Female||||',\n",
       "       'Female||Female||Female|||||||||||||', 'Female|||||||',\n",
       "       'Female|||Female||||||||||Female|||||',\n",
       "       'Female||Female||||||Female|', '|||||Female|Female||||',\n",
       "       '||||Female||||||||', '|||||Female||||||||||||',\n",
       "       '||||||||||||||||||||||||||||||||||Female||||||||||Female',\n",
       "       '||||||||||||Female||||', '||||||||||Female||', '||||Female||',\n",
       "       '|Female||||||||Female|||||||Female|||||||||Female||||',\n",
       "       '||||Female|||||||||||',\n",
       "       '||||||||||||||||||||||||||||||Female|Female||||||||||||||||||||||||||||||||||||||||||Female||||Female|||||||||Female|Female|||||||||||||||||Female||Female||||||||||||||||||||||||||||',\n",
       "       '||||||||||||||||Female||||||', '|Female||||',\n",
       "       '|||||Female|||||||||||||Female||||||||||||||||||||||||||',\n",
       "       '||||||||Female||||||||||||||||||||', 'Female|||||',\n",
       "       '||||||||Female|||||Female||||||||||||Female||Female|Female|||||||||||||||Female|||||||',\n",
       "       '|||Female||||||||||||||||', '|||Female|||||Female',\n",
       "       '||Female|||Female|||', '|Female|||||||||Female|',\n",
       "       '|||Female||||||Female|||||||Female||||',\n",
       "       'Female|||||Female||||||||||||||||Female|',\n",
       "       '||Female|Female||||Female|Female|||||||||Female|',\n",
       "       '||Female|Female||||||Female||Female||||',\n",
       "       '|||||||||||||||Female|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||Female||||||||',\n",
       "       '|||Female||||||||', '|||||||||Female|||||',\n",
       "       '|||||||Female|||||||', '||Female|Female||',\n",
       "       '||||Female||Female||||Female|||||', '|||||||||||||||||||||||',\n",
       "       '|||||||||||Female||||||||||Female||||||',\n",
       "       '||||||||||||||||||||Female||', '||||||||||||||||||Female||',\n",
       "       '|Female|||||||||||Female||||||',\n",
       "       '||Female||Female|Female||Female|Female||||Female||Female|Female||Female||Female|||',\n",
       "       'Female||||Female||||||Female||||Female||Female|||||Female||||||Female|Female|||Female|Female',\n",
       "       '||||Female|||||', '|||Female|||||||||||Female||||||||',\n",
       "       '|||||||||||||||Female|||', '|||Female|||Female|||',\n",
       "       '||||||||||||||||||||||||||||||||||||||||||||',\n",
       "       '||||||||||||||Female|||||||||||||||||||||||', '|Female|Female||',\n",
       "       'Female||||||Female', '||||||Female|||', '||Female||||||||',\n",
       "       '||Female|||||', '||||||||||Female|||||||Female|||||||||||||',\n",
       "       '||||||||||||Female|||||||||', '||||||||||||||||Female||||||||||',\n",
       "       '||||||||||||||||||||||||||||||Female||||||||||||||Female|||',\n",
       "       '||||||||||||Female||||||||Female|',\n",
       "       '|Female||||Female|Female||||||',\n",
       "       '|||||||||||||Female|||||||Female||',\n",
       "       '||||||||||||Female|||Female||',\n",
       "       '|Female||Female||||Female|Female|Female|Female|Female|Female|Female|Female|',\n",
       "       '||||||||||||Female|||||||||||Female||Female|Female||||||||||||Female||',\n",
       "       '|||||||||||||||||||||||||||||||||||||||||||',\n",
       "       'Female|Female||Female|',\n",
       "       'Female||||||||||||||Female|||||||||||||||||', '||Female|||||||',\n",
       "       'Female|||Female|||Female|', '|Female||||||||||||||||||||||||||||',\n",
       "       '|Female|||||||', '|||||Female||||||||||||||',\n",
       "       '||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||',\n",
       "       'Female||||Female|||Female|Female|Female|Female||||||Female||Female||Female|Female|||Female||Female|Female||Female|Female|Female||||Female||||',\n",
       "       '|||||||||Female||||||||', '|||||||||||||||||Female',\n",
       "       '||Female|||||||||', 'Female|Female|||',\n",
       "       '||||||Female|||||||||||||||||||Female|||||||||',\n",
       "       '|||||||Female|||||||||||||||',\n",
       "       '||||||||||||||||||||||||||||||||||||||||||||||||||||||||Female||||||||||||||||Female|||||||||||||||||||||||||||||||||||',\n",
       "       '|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||Female|||||||||||||Female|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||',\n",
       "       '|||||||||||||||||||||||||||||||Female||||||||',\n",
       "       '|Female||Female|Female|||||',\n",
       "       '|||||||||||||||||||||||||Female||||',\n",
       "       '|||||||||||||||||||||Female||||||',\n",
       "       '||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||',\n",
       "       'Female|||||||||||',\n",
       "       '||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||Female',\n",
       "       'Female||||Female', 'Female|||Female', 'Female||||Female|',\n",
       "       '|Female|Female|', '|||||||||||||||||Female||Female|||',\n",
       "       '||Female||||||||||||||||||||||||',\n",
       "       '|||||||||||||Female||||||||||||', 'Female||||||||||||||',\n",
       "       'Female|||||||||||||', 'Female|||||Female',\n",
       "       '||||||||||||||Female||Female|||||||', '|||||Female||||',\n",
       "       '||||||||||||||||||||Female||||||', '||Female|Female|',\n",
       "       'Female||Female|Female|Female||Female|Female||',\n",
       "       'Female|Female|Female|Female|Female|Female|Female||Female|Female|Female|Female|Female|Female|Female|Female|Female|Female||Female|Female|Female',\n",
       "       '|||Female|||||Female|', '|||||||||||Female', '|||Female|Female',\n",
       "       '||||Female||Female||||Female||', '||||||Female|',\n",
       "       '||||Female|||Female|Female|', 'Female|||Female||Female||||',\n",
       "       '|Female|Female|Female||||||', 'Female|Female|Female|||||||',\n",
       "       '|Female||Female||||Female||', '|||Female||Female|Female',\n",
       "       '|Female|Female||Female|||Female||Female', '|Female||Female|',\n",
       "       '||Female||Female', 'Female||Female||', '||Female|Female',\n",
       "       '||||||Female||||||||||||||||||||||', '||Female||Female|||',\n",
       "       '||||||||Female||||||',\n",
       "       'Female|Female|Female|Female|Female|Female|Female',\n",
       "       '||Female|||||||Female|', '|||Female||||',\n",
       "       'Female|||||Female||||Female|', '|Female||Female|||',\n",
       "       'Female||Female|', 'Female||Female|||', '||||Female|Female',\n",
       "       '|||||||||||||||||||||||||||||||Female||||||||Female|||||||||||Female||||||||||||||||||||||||||||||||||||',\n",
       "       '|||||Female|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||',\n",
       "       '|||||||||||||||||||||||||||Female|||||Female||||||||||||||||Female|||||||Female|||',\n",
       "       '||||||||||Female|||||||||||||||||||||||||||||Female|||||||||',\n",
       "       '||||||||Female|||Female||', '||||||||||||||||||||||||||||Female|',\n",
       "       'Female||||||||||||||||||Female||||||||||||||||||||||||||||||||Female|||||||||||||||||||||||||||||||||||||||||'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_data[0]['Gender_calculated'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save clean datasets in separate csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#i need to add more colums to the final dataset\n",
    "clean_data = []\n",
    "for df in transformed_data:\n",
    "    df = df[[ 'Artist', 'Title','Medium', 'Year_acquisition', 'Medium_classified']]\n",
    "    clean_data.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved met_clean.csv\n",
      "Saved reina_sofia_clean.csv\n",
      "Saved tate_clean.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Save each dataset using the corresponding name\n",
    "for dataset, name in zip(clean_data, dataset_names):\n",
    "    # Generate a unique filename using the dataset name\n",
    "    file_name = f\"{name}.csv\"\n",
    "    \n",
    "    # Save the DataFrame to a CSV file\n",
    "    dataset.to_csv(file_name, index=False)\n",
    "    print(f\"Saved {file_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
